{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import cv2\n",
    "import csv\n",
    "import os\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import mediapipe as mp \n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1725835513.281816  957293 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1725835513.290706 1052447 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1725835513.299404 1052447 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands = 2,\n",
    "    min_detection_confidence = 0.7,\n",
    "    min_tracking_confidence = 0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bounding_box(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    landmark_array = np.empty((0,2), int)\n",
    "\n",
    "    for _, landmark in enumerate(landmarks.landmark):\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "        landmark_point = [np.array((landmark_x, landmark_y))]\n",
    "\n",
    "        landmark_array = np.append(landmark_array, landmark_point, axis = 0)\n",
    "\n",
    "    x,y,w,h = cv2.boundingRect(landmark_array)\n",
    "    return [x,y,x+w,y+h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_box(image, brect):\n",
    "    cv2.rectangle(image,(brect[0], brect[1]), (brect[2], brect[3]), (0,0,0), 1)\n",
    "\n",
    "    image = depth_map_calculator(image, brect)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DPTForDepthEstimation were not initialized from the model checkpoint at Intel/dpt-large and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution1.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "estimator = pipeline(task = \"depth-estimation\", model = \"Intel/dpt-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_map_calculator(image, brect):\n",
    "\n",
    "    roi = image[brect[1]:brect[3], brect[0]:brect[2]]\n",
    "    roi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "    roi = PIL.Image.fromarray(roi)\n",
    "\n",
    "    output = estimator(images = roi)\n",
    "    depth_image = output['depth']\n",
    "    depth_image = np.asarray(depth_image)\n",
    "    depth_image = cv2.cvtColor(depth_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    image[brect[1]:brect[3], brect[0]:brect[2]] = depth_image\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m             brect \u001b[38;5;241m=\u001b[39m calc_bounding_box(debug_image, hand_landmarks)\n\u001b[1;32m     24\u001b[0m             \u001b[38;5;66;03m# Drawing bounding box\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m             debug_image \u001b[38;5;241m=\u001b[39m \u001b[43mdraw_bounding_box\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdebug_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbrect\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHand Gesture Recognition\u001b[39m\u001b[38;5;124m'\u001b[39m, debug_image)\n\u001b[1;32m     29\u001b[0m vid\u001b[38;5;241m.\u001b[39mrelease()\n",
      "Cell \u001b[0;32mIn[79], line 4\u001b[0m, in \u001b[0;36mdraw_bounding_box\u001b[0;34m(image, brect)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_bounding_box\u001b[39m(image, brect):\n\u001b[1;32m      2\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mrectangle(image,(brect[\u001b[38;5;241m0\u001b[39m], brect[\u001b[38;5;241m1\u001b[39m]), (brect[\u001b[38;5;241m2\u001b[39m], brect[\u001b[38;5;241m3\u001b[39m]), (\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mdepth_map_calculator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbrect\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "Cell \u001b[0;32mIn[75], line 4\u001b[0m, in \u001b[0;36mdepth_map_calculator\u001b[0;34m(image, brect)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdepth_map_calculator\u001b[39m(image, brect):\n\u001b[1;32m      3\u001b[0m     roi \u001b[38;5;241m=\u001b[39m image[brect[\u001b[38;5;241m1\u001b[39m]:brect[\u001b[38;5;241m3\u001b[39m], brect[\u001b[38;5;241m0\u001b[39m]:brect[\u001b[38;5;241m2\u001b[39m]]\n\u001b[0;32m----> 4\u001b[0m     roi \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2RGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     roi \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mfromarray(roi)\n\u001b[1;32m      7\u001b[0m     output \u001b[38;5;241m=\u001b[39m estimator(images \u001b[38;5;241m=\u001b[39m roi)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "vid = cv2.VideoCapture(0)\n",
    "while (True):\n",
    "\n",
    "    key = cv2.waitKey(10)\n",
    "    if key == 27:\n",
    "        break\n",
    "    # Camera capture\n",
    "    ret, image = vid.read()\n",
    "    # Stopping the loop camera stops detecting\n",
    "    if not ret:\n",
    "        break\n",
    "    # Flipping the image as the camera capture is mirrored.\n",
    "    image = cv2.flip(image,1)\n",
    "    debug_image = copy.deepcopy(image)\n",
    "    # Hand detection\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "\n",
    "            # Assigning bounding box\n",
    "            brect = calc_bounding_box(debug_image, hand_landmarks)\n",
    "            # Drawing bounding box\n",
    "            debug_image = draw_bounding_box(debug_image, brect)\n",
    "\n",
    "    cv2.imshow('Hand Gesture Recognition', debug_image)\n",
    "\n",
    "vid.release()\n",
    "cv2.destroyWindow('Hand Gesture Recognition')\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
